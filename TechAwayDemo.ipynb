{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TechAwayDemo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6N2Ihw3w7m9mD16nEj5zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elgonio/TechAwayThursdayDemo/blob/main/TechAwayDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeiTIaNSCJCM"
      },
      "source": [
        "# First we install the libraries we're using\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnLFddmWB8ft",
        "outputId": "48481ab7-3ba8-488c-f5dc-98ac379fe89c"
      },
      "source": [
        "!pip install tweepy\n",
        "!pip install nltk\n",
        "!pip install sklearn"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwtR7cSaDCMP"
      },
      "source": [
        "#Next we download the data we'll be training on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbqQio5UDAnJ",
        "outputId": "e255ee24-da2c-4669-eba0-a975ad1e2d8a"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "file_downloader = urllib.request.URLopener()\n",
        "file_downloader.retrieve(\"https://raw.githubusercontent.com/elgonio/TechAwayThursdayDemo/main/negative_tweets\", \"rt.neg\")\n",
        "file_downloader.retrieve(\"https://raw.githubusercontent.com/elgonio/TechAwayThursdayDemo/main/positive_tweets\", \"rt.pos\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('rt.pos', <http.client.HTTPMessage at 0x7f7b80d74b50>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8IssPNNGUnx"
      },
      "source": [
        "#Now we do the training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwam7H03BUK-",
        "outputId": "284d24fb-f0cd-4661-8894-ef1e021ab227"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\") # download if necessary\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# this part of the code is shamelessly borrowed from:\n",
        "# https://www.twilio.com/blog/2017/09/sentiment-analysis-python-messy-data-nltk.html\n",
        "def format_sentence(sent):\n",
        "    return({word: True for word in nltk.word_tokenize(sent)})\n",
        "\n",
        "pos = []\n",
        "with open(\"rt.pos\", encoding=\"latin-1\") as f:\n",
        "    for i in f:\n",
        "        if len(pos) > 4000: break\n",
        "        pos.append([format_sentence(i), 'pos'])\n",
        "\n",
        "\n",
        "neg = []\n",
        "with open(\"rt.neg\", encoding=\"latin-1\") as f:\n",
        "    for i in f:\n",
        "        if len(neg) > 4000: break\n",
        "        neg.append([format_sentence(i), 'neg'])\n",
        "\n",
        "# next, split labeled data into the training and test data\n",
        "training = pos[:int((.8)*len(pos))] + neg[:int((.8)*len(neg))]\n",
        "test = pos[int((.8)*len(pos)):] + neg[int((.8)*len(neg)):]\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(training)\n",
        "# We can show the most informative features to gain more insight into our data\n",
        "# classifier.show_most_informative_features()\n",
        "print(\"accuracy = \" + str(accuracy(classifier, test)))\n",
        "\n",
        "# training a model every time we run the program is inefiicient\n",
        "# we use pickle to store and save the model for future use\n",
        "classifier_file = open(\"classifier.pickle\", \"wb\")\n",
        "pickle.dump(classifier, classifier_file)\n",
        "classifier_file.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "accuracy = 0.7053682896379525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_13Ei8hBMI_",
        "outputId": "6a880547-c75d-496c-e802-628b505e8421"
      },
      "source": [
        "import tweepy\n",
        "\n",
        "# plz don't steal my twitter credentials\n",
        "ckey = \"nYKSjSpE5IrO9y7qECVk4jMMu\"\n",
        "csecret = \"YdCeEYJSGHVscthswLnNiDHmM7N24hSM82hslTVsRpgmn9evD3\"\n",
        "atoken = \"282101457-b7VzdJJnzNYzHrS6VwC7OCOzHOIKjedYI9NBP3ts\"\n",
        "asecret = \"MRSCxDqpIxdkGHj6SDAdh693tYxkTyyKPFS9TaKBI8fHq\"\n",
        "\n",
        "OAUTH_KEYS = {'consumer_key':ckey, 'consumer_secret':csecret,\n",
        "'access_token_key':atoken, 'access_token_secret':asecret}\n",
        "auth = tweepy.OAuthHandler(OAUTH_KEYS['consumer_key'], OAUTH_KEYS['consumer_secret'])\n",
        "api =tweepy.API(auth)\n",
        "\n",
        "query = input(\"What do you want to search for? \\n\")\n",
        "print('Searching...')\n",
        "tweets = tweepy.Cursor(api.search, q=query, lang='en', rpp='100').items(100)\n",
        "tweet_list = []\n",
        "for tweet in tweets:\n",
        "   # we append a new line for when we save to a file\n",
        "   tweet_list.append(tweet.text.strip() + \"\\n\")\n",
        "\n",
        "f=open('tweet_data.txt','w', encoding=\"utf-8\")\n",
        "f.writelines(tweet_list)\n",
        "print(\"fetched \" + str(len(tweet_list)) + \" tweets\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What do you want to search for? \n",
            "Eskom\n",
            "Searching...\n",
            "fetched 100 tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_tpX9dDGkQW"
      },
      "source": [
        "# Now we've fetched the tweets, let's run them through our classifier and see how people feel about our search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FD9IsolGZUV",
        "outputId": "25e2a48a-0b9b-41b9-a163-0b238d1b47b2"
      },
      "source": [
        "classifier_file = open(\"classifier.pickle\", \"rb\")\n",
        "classifier = pickle.load(classifier_file)\n",
        "classifier_file.close()\n",
        "\n",
        "score = 0\n",
        "for tweet in tweet_list:\n",
        "    tweet.rstrip()\n",
        "    tweet = tweet.replace(\"\\r\", \"\")\n",
        "    tweet = tweet.replace(\"\\n\", \"\")\n",
        "    # print(tweet + \" : \" + classifier.classify(format_sentence(tweet)))\n",
        "    if classifier.classify(format_sentence(tweet)) == \"pos\":\n",
        "        score = score + 1\n",
        "print(\"num tweets is: \", len(tweet_list))\n",
        "print(\"num pos tweets is: \", score)\n",
        "print(\"num neg tweets is: \", len(tweet_list)-score)\n",
        "print(\"score is : \", (score/len(tweet_list))*100, \"% positive\")\n",
        "if (score/len(tweet_list) > 0.8):\n",
        "    label = \"Very positive\"\n",
        "elif (score/len(tweet_list) > 0.6):\n",
        "    label = \"positive\"\n",
        "elif (score/len(tweet_list) > 0.4):\n",
        "    label = \"Neutral/Mixed\"\n",
        "elif (score/len(tweet_list) > 0.2):\n",
        "    label = \"Negative\"\n",
        "elif (score/len(tweet_list) > 0):\n",
        "    label = \"Very Negative\"\n",
        "\n",
        "print(\"general sentiment is %s\" % label)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num tweets is:  100\n",
            "num pos tweets is:  21\n",
            "num neg tweets is:  79\n",
            "score is :  21.0 % positive\n",
            "general sentiment is Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKb-zrSrHsLE"
      },
      "source": [
        "# scikit-learn example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SpMJWr3GM75"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEULpXA5GJ6O"
      },
      "source": [
        "cats = [[np.random.randint(5), np.random.randint(2), np.random.randint(7)] for i in range(1000)]\n",
        "labels = [0 if cats[i][1] == False else 1 for i in range(len(cats))]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNwAigj2-VCc",
        "outputId": "796a6f44-d38b-48ba-f4e2-68806d2edf02"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "X = cats\n",
        "Y = labels\n",
        "classifier = MLPClassifier()\n",
        "classifier.fit(X,Y)\n",
        "\n",
        "classifier.score(X,Y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZbp3c7E_BPB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}